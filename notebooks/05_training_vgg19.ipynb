{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Vgg19 using Adam Optim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.util import find_spec\n",
    "if find_spec(\"vgg\") is None:\n",
    "    import sys\n",
    "    sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "from vgg.data_loader.data_loaders import DefaultCifar100DataLoader\n",
    "from vgg.model.model import Vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "dataloader = DefaultCifar100DataLoader('../data', 32, validation_split=0.1)\n",
    "model = Vgg19(num_classes=100)\n",
    "model.to(device)\n",
    "trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(trainable_params, lr=10e-3, amsgrad=True)\n",
    "criterion = F.nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step = int(np.sqrt(dataloader.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, Loss: 4.605502\n",
      "Train Epoch: 0, Loss: 4.604686\n",
      "Train Epoch: 0, Loss: 4.601483\n",
      "Train Epoch: 0, Loss: 4.610888\n",
      "Train Epoch: 0, Loss: 4.602176\n",
      "Train Epoch: 0, Loss: 4.609723\n",
      "Train Epoch: 0, Loss: 4.604043\n",
      "Train Epoch: 0, Loss: 4.603904\n",
      "Train Epoch: 0, Loss: 4.598812\n",
      "Train Epoch: 0, Loss: 4.603806\n",
      "Train Epoch: 0, Loss: 4.603654\n",
      "Train Epoch: 0, Loss: 4.603394\n",
      "Train Epoch: 0, Loss: 4.609054\n",
      "Train Epoch: 0, Loss: 4.608366\n",
      "Train Epoch: 0, Loss: 4.601371\n",
      "Train Epoch: 0, Loss: 4.605363\n",
      "Train Epoch: 0, Loss: 4.607033\n",
      "Train Epoch: 0, Loss: 4.603831\n",
      "Train Epoch: 0, Loss: 4.608273\n",
      "Train Epoch: 0, Loss: 4.608895\n",
      "Train Epoch: 0, Loss: 4.604155\n",
      "Train Epoch: 0, Loss: 4.603164\n",
      "Train Epoch: 0, Loss: 4.608704\n",
      "Train Epoch: 0, Loss: 4.596412\n",
      "Train Epoch: 0, Loss: 4.599151\n",
      "Train Epoch: 0, Loss: 4.598418\n",
      "Train Epoch: 0, Loss: 4.597431\n",
      "Train Epoch: 0, Loss: 4.611198\n",
      "Train Epoch: 0, Loss: 4.610008\n",
      "Train Epoch: 0, Loss: 4.608357\n",
      "Train Epoch: 0, Loss: 4.604828\n",
      "Train Epoch: 0, Loss: 4.610844\n",
      "Train Epoch: 0, Loss: 4.601097\n",
      "Train Epoch: 0, Loss: 4.598757\n",
      "Train Epoch: 0, Loss: 4.603173\n",
      "Train Epoch: 0, Loss: 4.597926\n",
      "Train Epoch: 0, Loss: 4.601924\n",
      "Train Epoch: 0, Loss: 4.603784\n",
      "Train Epoch: 0, Loss: 4.607292\n",
      "Train Epoch: 0, Loss: 4.600620\n",
      "Train Epoch: 0, Loss: 4.610021\n",
      "Train Epoch: 0, Loss: 4.602178\n",
      "Train Epoch: 0, Loss: 4.608755\n",
      "Train Epoch: 0, Loss: 4.609342\n",
      "Train Epoch: 0, Loss: 4.606919\n",
      "Train Epoch: 0, Loss: 4.597623\n",
      "Train Epoch: 0, Loss: 4.610641\n",
      "Train Epoch: 0, Loss: 4.604297\n",
      "Train Epoch: 0, Loss: 4.612138\n",
      "Train Epoch: 0, Loss: 4.605018\n",
      "Train Epoch: 0, Loss: 4.605419\n",
      "Train Epoch: 0, Loss: 4.600079\n",
      "Train Epoch: 0, Loss: 4.605874\n",
      "Train Epoch: 0, Loss: 4.607107\n",
      "Train Epoch: 0, Loss: 4.610897\n",
      "Train Epoch: 0, Loss: 4.610043\n",
      "Train Epoch: 0, Loss: 4.603520\n",
      "Train Epoch: 0, Loss: 4.615241\n",
      "Train Epoch: 0, Loss: 4.600287\n",
      "Train Epoch: 0, Loss: 4.598858\n",
      "Train Epoch: 0, Loss: 4.608578\n",
      "Train Epoch: 0, Loss: 4.603864\n",
      "Train Epoch: 0, Loss: 4.606150\n",
      "Train Epoch: 0, Loss: 4.614338\n",
      "Train Epoch: 0, Loss: 4.610866\n",
      "Train Epoch: 0, Loss: 4.604477\n",
      "Train Epoch: 0, Loss: 4.609013\n",
      "Train Epoch: 0, Loss: 4.599431\n",
      "Train Epoch: 0, Loss: 4.608930\n",
      "Train Epoch: 0, Loss: 4.598367\n",
      "Train Epoch: 0, Loss: 4.614204\n",
      "Train Epoch: 0, Loss: 4.604407\n",
      "Train Epoch: 0, Loss: 4.599358\n",
      "Train Epoch: 0, Loss: 4.602390\n",
      "Train Epoch: 0, Loss: 4.604959\n",
      "Train Epoch: 0, Loss: 4.599486\n",
      "Train Epoch: 0, Loss: 4.597723\n",
      "Train Epoch: 0, Loss: 4.596887\n",
      "Train Epoch: 0, Loss: 4.605608\n",
      "Train Epoch: 0, Loss: 4.610754\n",
      "Train Epoch: 0, Loss: 4.601010\n",
      "Train Epoch: 0, Loss: 4.604841\n",
      "Train Epoch: 0, Loss: 4.609138\n",
      "Train Epoch: 0, Loss: 4.611876\n",
      "Train Epoch: 0, Loss: 4.609491\n",
      "Train Epoch: 0, Loss: 4.612428\n",
      "Train Epoch: 0, Loss: 4.606315\n",
      "Train Epoch: 0, Loss: 4.612024\n",
      "Train Epoch: 0, Loss: 4.605203\n",
      "Train Epoch: 0, Loss: 4.599735\n",
      "Train Epoch: 0, Loss: 4.597082\n",
      "Train Epoch: 0, Loss: 4.607566\n",
      "Train Epoch: 0, Loss: 4.606546\n",
      "Train Epoch: 0, Loss: 4.609560\n",
      "Train Epoch: 0, Loss: 4.606078\n",
      "Train Epoch: 0, Loss: 4.603663\n",
      "Train Epoch: 0, Loss: 4.606456\n",
      "Train Epoch: 0, Loss: 4.607712\n",
      "Train Epoch: 0, Loss: 4.603863\n",
      "Train Epoch: 0, Loss: 4.603467\n",
      "Train Epoch: 0, Loss: 4.603719\n",
      "Train Epoch: 0, Loss: 4.609975\n",
      "Train Epoch: 0, Loss: 4.601477\n",
      "Train Epoch: 0, Loss: 4.610885\n",
      "Train Epoch: 0, Loss: 4.606892\n",
      "Train Epoch: 0, Loss: 4.613248\n",
      "Train Epoch: 0, Loss: 4.610873\n",
      "Train Epoch: 0, Loss: 4.612899\n",
      "Train Epoch: 0, Loss: 4.608827\n",
      "Train Epoch: 0, Loss: 4.600157\n",
      "Train Epoch: 0, Loss: 4.600317\n",
      "Train Epoch: 0, Loss: 4.603159\n",
      "Train Epoch: 0, Loss: 4.604454\n",
      "Train Epoch: 0, Loss: 4.602942\n",
      "Train Epoch: 0, Loss: 4.609934\n",
      "Train Epoch: 0, Loss: 4.609871\n",
      "Train Epoch: 0, Loss: 4.607860\n",
      "Train Epoch: 0, Loss: 4.599482\n",
      "Train Epoch: 0, Loss: 4.594809\n",
      "Train Epoch: 0, Loss: 4.601151\n",
      "Train Epoch: 0, Loss: 4.610802\n",
      "Train Epoch: 0, Loss: 4.600833\n",
      "Train Epoch: 0, Loss: 4.611550\n",
      "Train Epoch: 0, Loss: 4.601320\n",
      "Train Epoch: 0, Loss: 4.610018\n",
      "Train Epoch: 0, Loss: 4.606038\n",
      "Train Epoch: 0, Loss: 4.597827\n",
      "Train Epoch: 0, Loss: 4.602681\n",
      "Train Epoch: 0, Loss: 4.614089\n",
      "Train Epoch: 0, Loss: 4.609601\n",
      "Train Epoch: 0, Loss: 4.608929\n",
      "Train Epoch: 0, Loss: 4.607728\n",
      "Train Epoch: 0, Loss: 4.604137\n",
      "Train Epoch: 0, Loss: 4.607013\n",
      "Train Epoch: 0, Loss: 4.599401\n",
      "Train Epoch: 0, Loss: 4.609454\n",
      "Train Epoch: 0, Loss: 4.603508\n",
      "Train Epoch: 0, Loss: 4.603821\n",
      "Train Epoch: 0, Loss: 4.604633\n",
      "Train Epoch: 0, Loss: 4.606549\n",
      "Train Epoch: 0, Loss: 4.604392\n",
      "Train Epoch: 0, Loss: 4.615379\n",
      "Train Epoch: 0, Loss: 4.599320\n",
      "Train Epoch: 0, Loss: 4.603413\n",
      "Train Epoch: 0, Loss: 4.603328\n",
      "Train Epoch: 0, Loss: 4.608474\n",
      "Train Epoch: 0, Loss: 4.602611\n",
      "Train Epoch: 0, Loss: 4.597574\n",
      "Train Epoch: 0, Loss: 4.603243\n",
      "Train Epoch: 0, Loss: 4.603263\n",
      "Train Epoch: 0, Loss: 4.605431\n",
      "Train Epoch: 0, Loss: 4.607823\n",
      "Train Epoch: 0, Loss: 4.609957\n",
      "Train Epoch: 0, Loss: 4.609446\n",
      "Train Epoch: 0, Loss: 4.601968\n",
      "Train Epoch: 0, Loss: 4.605732\n",
      "Train Epoch: 0, Loss: 4.608456\n",
      "Train Epoch: 0, Loss: 4.605187\n",
      "Train Epoch: 0, Loss: 4.606957\n",
      "Train Epoch: 0, Loss: 4.601523\n",
      "Train Epoch: 0, Loss: 4.605191\n",
      "Train Epoch: 0, Loss: 4.609392\n",
      "Train Epoch: 0, Loss: 4.606590\n",
      "Train Epoch: 0, Loss: 4.608212\n",
      "Train Epoch: 0, Loss: 4.603453\n",
      "Train Epoch: 0, Loss: 4.602765\n",
      "Train Epoch: 0, Loss: 4.599401\n",
      "Train Epoch: 0, Loss: 4.606555\n",
      "Train Epoch: 0, Loss: 4.613626\n",
      "Train Epoch: 0, Loss: 4.603766\n",
      "Train Epoch: 0, Loss: 4.604554\n",
      "Train Epoch: 0, Loss: 4.608177\n",
      "Train Epoch: 0, Loss: 4.602488\n",
      "Train Epoch: 0, Loss: 4.607209\n",
      "Train Epoch: 0, Loss: 4.609155\n",
      "Train Epoch: 0, Loss: 4.602607\n",
      "Train Epoch: 0, Loss: 4.598774\n",
      "Train Epoch: 0, Loss: 4.600842\n",
      "Train Epoch: 0, Loss: 4.606297\n",
      "Train Epoch: 0, Loss: 4.606195\n",
      "Train Epoch: 0, Loss: 4.600670\n",
      "Train Epoch: 0, Loss: 4.603077\n",
      "Train Epoch: 0, Loss: 4.602499\n",
      "Train Epoch: 0, Loss: 4.605941\n",
      "Train Epoch: 0, Loss: 4.612890\n",
      "Train Epoch: 0, Loss: 4.610363\n",
      "Train Epoch: 0, Loss: 4.612588\n",
      "Train Epoch: 0, Loss: 4.605854\n",
      "Train Epoch: 0, Loss: 4.609812\n",
      "Train Epoch: 0, Loss: 4.602874\n",
      "Train Epoch: 0, Loss: 4.607800\n",
      "Train Epoch: 0, Loss: 4.599742\n",
      "Train Epoch: 0, Loss: 4.604062\n",
      "Train Epoch: 0, Loss: 4.609559\n",
      "Train Epoch: 0, Loss: 4.607228\n",
      "Train Epoch: 0, Loss: 4.605215\n",
      "Train Epoch: 0, Loss: 4.602239\n",
      "Train Epoch: 0, Loss: 4.607925\n",
      "Train Epoch: 0, Loss: 4.608177\n",
      "Train Epoch: 0, Loss: 4.610121\n",
      "Train Epoch: 0, Loss: 4.614635\n",
      "Train Epoch: 0, Loss: 4.603896\n",
      "Train Epoch: 0, Loss: 4.600735\n",
      "Train Epoch: 0, Loss: 4.603446\n",
      "Train Epoch: 0, Loss: 4.612590\n",
      "Train Epoch: 0, Loss: 4.603241\n",
      "Train Epoch: 0, Loss: 4.601275\n",
      "Train Epoch: 0, Loss: 4.604221\n",
      "Train Epoch: 0, Loss: 4.606513\n",
      "Train Epoch: 0, Loss: 4.602292\n",
      "Train Epoch: 0, Loss: 4.602348\n",
      "Train Epoch: 0, Loss: 4.608061\n",
      "Train Epoch: 0, Loss: 4.611797\n",
      "Train Epoch: 0, Loss: 4.607294\n",
      "Train Epoch: 0, Loss: 4.606745\n",
      "Train Epoch: 0, Loss: 4.604425\n",
      "Train Epoch: 0, Loss: 4.608614\n",
      "Train Epoch: 0, Loss: 4.602632\n",
      "Train Epoch: 0, Loss: 4.600490\n",
      "Train Epoch: 0, Loss: 4.609468\n",
      "Train Epoch: 0, Loss: 4.607215\n",
      "Train Epoch: 0, Loss: 4.608224\n",
      "Train Epoch: 0, Loss: 4.607053\n",
      "Train Epoch: 0, Loss: 4.606130\n",
      "Train Epoch: 0, Loss: 4.611895\n",
      "Train Epoch: 0, Loss: 4.612511\n",
      "Train Epoch: 0, Loss: 4.605174\n",
      "Train Epoch: 0, Loss: 4.605378\n",
      "Train Epoch: 0, Loss: 4.604730\n",
      "Train Epoch: 0, Loss: 4.607364\n",
      "Train Epoch: 0, Loss: 4.598948\n",
      "Train Epoch: 0, Loss: 4.610957\n",
      "Train Epoch: 0, Loss: 4.599299\n",
      "Train Epoch: 0, Loss: 4.607728\n",
      "Train Epoch: 0, Loss: 4.607410\n",
      "Train Epoch: 0, Loss: 4.607489\n",
      "Train Epoch: 0, Loss: 4.601635\n",
      "Train Epoch: 0, Loss: 4.602800\n",
      "Train Epoch: 0, Loss: 4.602114\n",
      "Train Epoch: 0, Loss: 4.610501\n",
      "Train Epoch: 0, Loss: 4.608034\n",
      "Train Epoch: 0, Loss: 4.608094\n",
      "Train Epoch: 0, Loss: 4.607024\n",
      "Train Epoch: 0, Loss: 4.599817\n",
      "Train Epoch: 0, Loss: 4.604862\n",
      "Train Epoch: 0, Loss: 4.602158\n",
      "Train Epoch: 0, Loss: 4.607510\n",
      "Train Epoch: 0, Loss: 4.601304\n",
      "Train Epoch: 0, Loss: 4.600176\n",
      "Train Epoch: 0, Loss: 4.608448\n",
      "Train Epoch: 0, Loss: 4.601800\n",
      "Train Epoch: 0, Loss: 4.612443\n",
      "Train Epoch: 0, Loss: 4.613191\n",
      "Train Epoch: 0, Loss: 4.604820\n",
      "Train Epoch: 0, Loss: 4.605041\n",
      "Train Epoch: 0, Loss: 4.601390\n",
      "Train Epoch: 0, Loss: 4.591293\n",
      "Train Epoch: 0, Loss: 4.610179\n",
      "Train Epoch: 0, Loss: 4.610085\n",
      "Train Epoch: 0, Loss: 4.597416\n",
      "Train Epoch: 0, Loss: 4.602009\n",
      "Train Epoch: 0, Loss: 4.608878\n",
      "Train Epoch: 0, Loss: 4.608928\n",
      "Train Epoch: 0, Loss: 4.603638\n",
      "Train Epoch: 0, Loss: 4.603137\n",
      "Train Epoch: 0, Loss: 4.607975\n",
      "Train Epoch: 0, Loss: 4.611606\n",
      "Train Epoch: 0, Loss: 4.600134\n",
      "Train Epoch: 0, Loss: 4.606482\n",
      "Train Epoch: 0, Loss: 4.608712\n",
      "Train Epoch: 0, Loss: 4.609052\n",
      "Train Epoch: 0, Loss: 4.599942\n",
      "Train Epoch: 0, Loss: 4.606573\n",
      "Train Epoch: 0, Loss: 4.606611\n",
      "Train Epoch: 0, Loss: 4.599794\n",
      "Train Epoch: 0, Loss: 4.606364\n",
      "Train Epoch: 0, Loss: 4.602890\n",
      "Train Epoch: 0, Loss: 4.597872\n",
      "Train Epoch: 0, Loss: 4.607135\n",
      "Train Epoch: 0, Loss: 4.605409\n",
      "Train Epoch: 0, Loss: 4.604703\n",
      "Train Epoch: 0, Loss: 4.601393\n",
      "Train Epoch: 1, Loss: 4.605552\n",
      "Train Epoch: 1, Loss: 4.611092\n",
      "Train Epoch: 1, Loss: 4.608675\n",
      "Train Epoch: 1, Loss: 4.608121\n",
      "Train Epoch: 1, Loss: 4.604693\n",
      "Train Epoch: 1, Loss: 4.607527\n",
      "Train Epoch: 1, Loss: 4.612192\n",
      "Train Epoch: 1, Loss: 4.610035\n",
      "Train Epoch: 1, Loss: 4.606277\n",
      "Train Epoch: 1, Loss: 4.602916\n",
      "Train Epoch: 1, Loss: 4.604726\n",
      "Train Epoch: 1, Loss: 4.615026\n",
      "Train Epoch: 1, Loss: 4.602763\n",
      "Train Epoch: 1, Loss: 4.601870\n",
      "Train Epoch: 1, Loss: 4.597890\n",
      "Train Epoch: 1, Loss: 4.604401\n",
      "Train Epoch: 1, Loss: 4.606424\n",
      "Train Epoch: 1, Loss: 4.600746\n",
      "Train Epoch: 1, Loss: 4.611427\n",
      "Train Epoch: 1, Loss: 4.608312\n",
      "Train Epoch: 1, Loss: 4.607969\n",
      "Train Epoch: 1, Loss: 4.609436\n",
      "Train Epoch: 1, Loss: 4.605895\n",
      "Train Epoch: 1, Loss: 4.611519\n",
      "Train Epoch: 1, Loss: 4.609008\n",
      "Train Epoch: 1, Loss: 4.609217\n",
      "Train Epoch: 1, Loss: 4.606796\n",
      "Train Epoch: 1, Loss: 4.609142\n",
      "Train Epoch: 1, Loss: 4.607300\n",
      "Train Epoch: 1, Loss: 4.605054\n",
      "Train Epoch: 1, Loss: 4.607502\n",
      "Train Epoch: 1, Loss: 4.609339\n",
      "Train Epoch: 1, Loss: 4.606069\n",
      "Train Epoch: 1, Loss: 4.608640\n",
      "Train Epoch: 1, Loss: 4.608534\n",
      "Train Epoch: 1, Loss: 4.604187\n",
      "Train Epoch: 1, Loss: 4.608697\n",
      "Train Epoch: 1, Loss: 4.605140\n",
      "Train Epoch: 1, Loss: 4.610296\n",
      "Train Epoch: 1, Loss: 4.606278\n",
      "Train Epoch: 1, Loss: 4.603653\n",
      "Train Epoch: 1, Loss: 4.604791\n",
      "Train Epoch: 1, Loss: 4.609482\n",
      "Train Epoch: 1, Loss: 4.605235\n",
      "Train Epoch: 1, Loss: 4.606138\n",
      "Train Epoch: 1, Loss: 4.600122\n",
      "Train Epoch: 1, Loss: 4.604740\n",
      "Train Epoch: 1, Loss: 4.610722\n",
      "Train Epoch: 1, Loss: 4.600783\n",
      "Train Epoch: 1, Loss: 4.609180\n",
      "Train Epoch: 1, Loss: 4.605944\n",
      "Train Epoch: 1, Loss: 4.604280\n",
      "Train Epoch: 1, Loss: 4.605886\n",
      "Train Epoch: 1, Loss: 4.603091\n",
      "Train Epoch: 1, Loss: 4.607884\n",
      "Train Epoch: 1, Loss: 4.603284\n",
      "Train Epoch: 1, Loss: 4.603360\n",
      "Train Epoch: 1, Loss: 4.600337\n",
      "Train Epoch: 1, Loss: 4.608203\n",
      "Train Epoch: 1, Loss: 4.609572\n",
      "Train Epoch: 1, Loss: 4.599864\n",
      "Train Epoch: 1, Loss: 4.606301\n",
      "Train Epoch: 1, Loss: 4.601189\n",
      "Train Epoch: 1, Loss: 4.606587\n",
      "Train Epoch: 1, Loss: 4.605127\n",
      "Train Epoch: 1, Loss: 4.597353\n",
      "Train Epoch: 1, Loss: 4.608994\n",
      "Train Epoch: 1, Loss: 4.607139\n",
      "Train Epoch: 1, Loss: 4.606267\n",
      "Train Epoch: 1, Loss: 4.599925\n",
      "Train Epoch: 1, Loss: 4.604109\n",
      "Train Epoch: 1, Loss: 4.604592\n",
      "Train Epoch: 1, Loss: 4.603618\n",
      "Train Epoch: 1, Loss: 4.609137\n",
      "Train Epoch: 1, Loss: 4.605939\n",
      "Train Epoch: 1, Loss: 4.606133\n",
      "Train Epoch: 1, Loss: 4.601753\n",
      "Train Epoch: 1, Loss: 4.605719\n",
      "Train Epoch: 1, Loss: 4.596000\n",
      "Train Epoch: 1, Loss: 4.605251\n",
      "Train Epoch: 1, Loss: 4.600427\n",
      "Train Epoch: 1, Loss: 4.605128\n",
      "Train Epoch: 1, Loss: 4.606969\n",
      "Train Epoch: 1, Loss: 4.610072\n",
      "Train Epoch: 1, Loss: 4.606989\n",
      "Train Epoch: 1, Loss: 4.606213\n",
      "Train Epoch: 1, Loss: 4.603741\n",
      "Train Epoch: 1, Loss: 4.608601\n",
      "Train Epoch: 1, Loss: 4.606446\n",
      "Train Epoch: 1, Loss: 4.600726\n",
      "Train Epoch: 1, Loss: 4.607436\n",
      "Train Epoch: 1, Loss: 4.605407\n",
      "Train Epoch: 1, Loss: 4.597150\n",
      "Train Epoch: 1, Loss: 4.608065\n",
      "Train Epoch: 1, Loss: 4.603609\n",
      "Train Epoch: 1, Loss: 4.609852\n",
      "Train Epoch: 1, Loss: 4.602340\n",
      "Train Epoch: 1, Loss: 4.609650\n",
      "Train Epoch: 1, Loss: 4.608944\n",
      "Train Epoch: 1, Loss: 4.610384\n",
      "Train Epoch: 1, Loss: 4.602070\n",
      "Train Epoch: 1, Loss: 4.605989\n",
      "Train Epoch: 1, Loss: 4.607983\n",
      "Train Epoch: 1, Loss: 4.607918\n",
      "Train Epoch: 1, Loss: 4.606436\n",
      "Train Epoch: 1, Loss: 4.601207\n",
      "Train Epoch: 1, Loss: 4.610413\n",
      "Train Epoch: 1, Loss: 4.613925\n",
      "Train Epoch: 1, Loss: 4.608496\n",
      "Train Epoch: 1, Loss: 4.604420\n",
      "Train Epoch: 1, Loss: 4.604814\n",
      "Train Epoch: 1, Loss: 4.609742\n",
      "Train Epoch: 1, Loss: 4.605116\n",
      "Train Epoch: 1, Loss: 4.607627\n",
      "Train Epoch: 1, Loss: 4.603989\n",
      "Train Epoch: 1, Loss: 4.613005\n",
      "Train Epoch: 1, Loss: 4.608538\n",
      "Train Epoch: 1, Loss: 4.607737\n",
      "Train Epoch: 1, Loss: 4.605963\n",
      "Train Epoch: 1, Loss: 4.615285\n",
      "Train Epoch: 1, Loss: 4.605994\n",
      "Train Epoch: 1, Loss: 4.597441\n",
      "Train Epoch: 1, Loss: 4.614728\n",
      "Train Epoch: 1, Loss: 4.600714\n",
      "Train Epoch: 1, Loss: 4.610891\n",
      "Train Epoch: 1, Loss: 4.613231\n",
      "Train Epoch: 1, Loss: 4.606821\n",
      "Train Epoch: 1, Loss: 4.596358\n",
      "Train Epoch: 1, Loss: 4.606717\n",
      "Train Epoch: 1, Loss: 4.607318\n",
      "Train Epoch: 1, Loss: 4.599288\n",
      "Train Epoch: 1, Loss: 4.597633\n",
      "Train Epoch: 1, Loss: 4.598661\n",
      "Train Epoch: 1, Loss: 4.606544\n",
      "Train Epoch: 1, Loss: 4.602326\n",
      "Train Epoch: 1, Loss: 4.606336\n",
      "Train Epoch: 1, Loss: 4.604687\n",
      "Train Epoch: 1, Loss: 4.603794\n",
      "Train Epoch: 1, Loss: 4.602734\n",
      "Train Epoch: 1, Loss: 4.605038\n",
      "Train Epoch: 1, Loss: 4.606032\n",
      "Train Epoch: 1, Loss: 4.601828\n",
      "Train Epoch: 1, Loss: 4.602532\n",
      "Train Epoch: 1, Loss: 4.606717\n",
      "Train Epoch: 1, Loss: 4.602500\n",
      "Train Epoch: 1, Loss: 4.608388\n",
      "Train Epoch: 1, Loss: 4.605218\n",
      "Train Epoch: 1, Loss: 4.600896\n",
      "Train Epoch: 1, Loss: 4.608657\n",
      "Train Epoch: 1, Loss: 4.608130\n",
      "Train Epoch: 1, Loss: 4.604637\n",
      "Train Epoch: 1, Loss: 4.614536\n",
      "Train Epoch: 1, Loss: 4.599773\n",
      "Train Epoch: 1, Loss: 4.600057\n",
      "Train Epoch: 1, Loss: 4.598164\n",
      "Train Epoch: 1, Loss: 4.600901\n",
      "Train Epoch: 1, Loss: 4.606835\n",
      "Train Epoch: 1, Loss: 4.602368\n",
      "Train Epoch: 1, Loss: 4.607779\n",
      "Train Epoch: 1, Loss: 4.609930\n",
      "Train Epoch: 1, Loss: 4.611733\n",
      "Train Epoch: 1, Loss: 4.608924\n",
      "Train Epoch: 1, Loss: 4.611715\n",
      "Train Epoch: 1, Loss: 4.604322\n",
      "Train Epoch: 1, Loss: 4.613242\n",
      "Train Epoch: 1, Loss: 4.604961\n",
      "Train Epoch: 1, Loss: 4.603449\n",
      "Train Epoch: 1, Loss: 4.612327\n",
      "Train Epoch: 1, Loss: 4.593243\n",
      "Train Epoch: 1, Loss: 4.598970\n",
      "Train Epoch: 1, Loss: 4.605215\n",
      "Train Epoch: 1, Loss: 4.610982\n",
      "Train Epoch: 1, Loss: 4.605814\n",
      "Train Epoch: 1, Loss: 4.603355\n",
      "Train Epoch: 1, Loss: 4.604981\n",
      "Train Epoch: 1, Loss: 4.600150\n",
      "Train Epoch: 1, Loss: 4.607956\n",
      "Train Epoch: 1, Loss: 4.608078\n",
      "Train Epoch: 1, Loss: 4.605745\n",
      "Train Epoch: 1, Loss: 4.603023\n",
      "Train Epoch: 1, Loss: 4.605933\n",
      "Train Epoch: 1, Loss: 4.608958\n",
      "Train Epoch: 1, Loss: 4.599856\n",
      "Train Epoch: 1, Loss: 4.612057\n",
      "Train Epoch: 1, Loss: 4.609565\n",
      "Train Epoch: 1, Loss: 4.609099\n",
      "Train Epoch: 1, Loss: 4.612596\n",
      "Train Epoch: 1, Loss: 4.598812\n",
      "Train Epoch: 1, Loss: 4.601848\n",
      "Train Epoch: 1, Loss: 4.602561\n",
      "Train Epoch: 1, Loss: 4.607178\n",
      "Train Epoch: 1, Loss: 4.612187\n",
      "Train Epoch: 1, Loss: 4.605408\n",
      "Train Epoch: 1, Loss: 4.601881\n",
      "Train Epoch: 1, Loss: 4.605706\n",
      "Train Epoch: 1, Loss: 4.611172\n",
      "Train Epoch: 1, Loss: 4.607930\n",
      "Train Epoch: 1, Loss: 4.610146\n",
      "Train Epoch: 1, Loss: 4.611139\n",
      "Train Epoch: 1, Loss: 4.601665\n",
      "Train Epoch: 1, Loss: 4.599226\n",
      "Train Epoch: 1, Loss: 4.607677\n",
      "Train Epoch: 1, Loss: 4.601228\n",
      "Train Epoch: 1, Loss: 4.603222\n",
      "Train Epoch: 1, Loss: 4.604086\n",
      "Train Epoch: 1, Loss: 4.614296\n",
      "Train Epoch: 1, Loss: 4.605963\n",
      "Train Epoch: 1, Loss: 4.606257\n",
      "Train Epoch: 1, Loss: 4.598326\n",
      "Train Epoch: 1, Loss: 4.605509\n",
      "Train Epoch: 1, Loss: 4.604759\n",
      "Train Epoch: 1, Loss: 4.598358\n",
      "Train Epoch: 1, Loss: 4.604618\n",
      "Train Epoch: 1, Loss: 4.608034\n",
      "Train Epoch: 1, Loss: 4.602727\n",
      "Train Epoch: 1, Loss: 4.606963\n",
      "Train Epoch: 1, Loss: 4.602063\n",
      "Train Epoch: 1, Loss: 4.608744\n",
      "Train Epoch: 1, Loss: 4.604185\n",
      "Train Epoch: 1, Loss: 4.606298\n",
      "Train Epoch: 1, Loss: 4.608777\n",
      "Train Epoch: 1, Loss: 4.605996\n",
      "Train Epoch: 1, Loss: 4.600331\n",
      "Train Epoch: 1, Loss: 4.600928\n",
      "Train Epoch: 1, Loss: 4.604975\n",
      "Train Epoch: 1, Loss: 4.606175\n",
      "Train Epoch: 1, Loss: 4.603846\n",
      "Train Epoch: 1, Loss: 4.603732\n",
      "Train Epoch: 1, Loss: 4.602438\n",
      "Train Epoch: 1, Loss: 4.601262\n",
      "Train Epoch: 1, Loss: 4.609248\n",
      "Train Epoch: 1, Loss: 4.606533\n",
      "Train Epoch: 1, Loss: 4.614738\n",
      "Train Epoch: 1, Loss: 4.614427\n",
      "Train Epoch: 1, Loss: 4.603203\n",
      "Train Epoch: 1, Loss: 4.608507\n",
      "Train Epoch: 1, Loss: 4.606857\n",
      "Train Epoch: 1, Loss: 4.599507\n",
      "Train Epoch: 1, Loss: 4.606801\n",
      "Train Epoch: 1, Loss: 4.596252\n",
      "Train Epoch: 1, Loss: 4.609018\n",
      "Train Epoch: 1, Loss: 4.604692\n",
      "Train Epoch: 1, Loss: 4.606992\n",
      "Train Epoch: 1, Loss: 4.606545\n",
      "Train Epoch: 1, Loss: 4.607355\n",
      "Train Epoch: 1, Loss: 4.608227\n",
      "Train Epoch: 1, Loss: 4.600215\n",
      "Train Epoch: 1, Loss: 4.601962\n",
      "Train Epoch: 1, Loss: 4.609344\n",
      "Train Epoch: 1, Loss: 4.606505\n",
      "Train Epoch: 1, Loss: 4.604372\n",
      "Train Epoch: 1, Loss: 4.604667\n",
      "Train Epoch: 1, Loss: 4.602988\n",
      "Train Epoch: 1, Loss: 4.607784\n",
      "Train Epoch: 1, Loss: 4.613367\n",
      "Train Epoch: 1, Loss: 4.606595\n",
      "Train Epoch: 1, Loss: 4.607110\n",
      "Train Epoch: 1, Loss: 4.612782\n",
      "Train Epoch: 1, Loss: 4.602541\n",
      "Train Epoch: 1, Loss: 4.603223\n",
      "Train Epoch: 1, Loss: 4.611845\n",
      "Train Epoch: 1, Loss: 4.608969\n",
      "Train Epoch: 1, Loss: 4.605443\n",
      "Train Epoch: 1, Loss: 4.603205\n",
      "Train Epoch: 1, Loss: 4.608603\n",
      "Train Epoch: 1, Loss: 4.605875\n",
      "Train Epoch: 1, Loss: 4.602213\n",
      "Train Epoch: 1, Loss: 4.600707\n",
      "Train Epoch: 1, Loss: 4.599905\n",
      "Train Epoch: 1, Loss: 4.601835\n",
      "Train Epoch: 1, Loss: 4.606947\n",
      "Train Epoch: 1, Loss: 4.605227\n",
      "Train Epoch: 1, Loss: 4.606162\n",
      "Train Epoch: 1, Loss: 4.600578\n",
      "Train Epoch: 1, Loss: 4.607475\n",
      "Train Epoch: 1, Loss: 4.609616\n",
      "Train Epoch: 1, Loss: 4.608254\n",
      "Train Epoch: 1, Loss: 4.615083\n",
      "Train Epoch: 1, Loss: 4.608901\n",
      "Train Epoch: 1, Loss: 4.606416\n",
      "Train Epoch: 1, Loss: 4.610482\n",
      "Train Epoch: 1, Loss: 4.601117\n",
      "Train Epoch: 2, Loss: 4.605293\n",
      "Train Epoch: 2, Loss: 4.597387\n",
      "Train Epoch: 2, Loss: 4.603912\n",
      "Train Epoch: 2, Loss: 4.608610\n",
      "Train Epoch: 2, Loss: 4.598044\n",
      "Train Epoch: 2, Loss: 4.596960\n",
      "Train Epoch: 2, Loss: 4.606385\n",
      "Train Epoch: 2, Loss: 4.604921\n",
      "Train Epoch: 2, Loss: 4.599693\n",
      "Train Epoch: 2, Loss: 4.614117\n",
      "Train Epoch: 2, Loss: 4.614624\n",
      "Train Epoch: 2, Loss: 4.608018\n",
      "Train Epoch: 2, Loss: 4.594695\n",
      "Train Epoch: 2, Loss: 4.608048\n",
      "Train Epoch: 2, Loss: 4.604137\n",
      "Train Epoch: 2, Loss: 4.608491\n",
      "Train Epoch: 2, Loss: 4.613084\n",
      "Train Epoch: 2, Loss: 4.607497\n",
      "Train Epoch: 2, Loss: 4.600050\n",
      "Train Epoch: 2, Loss: 4.598799\n",
      "Train Epoch: 2, Loss: 4.606579\n",
      "Train Epoch: 2, Loss: 4.604620\n",
      "Train Epoch: 2, Loss: 4.597603\n",
      "Train Epoch: 2, Loss: 4.602691\n",
      "Train Epoch: 2, Loss: 4.597909\n",
      "Train Epoch: 2, Loss: 4.609902\n",
      "Train Epoch: 2, Loss: 4.606576\n",
      "Train Epoch: 2, Loss: 4.605777\n",
      "Train Epoch: 2, Loss: 4.611403\n",
      "Train Epoch: 2, Loss: 4.601823\n",
      "Train Epoch: 2, Loss: 4.600078\n",
      "Train Epoch: 2, Loss: 4.606361\n",
      "Train Epoch: 2, Loss: 4.604398\n",
      "Train Epoch: 2, Loss: 4.600927\n",
      "Train Epoch: 2, Loss: 4.604919\n",
      "Train Epoch: 2, Loss: 4.598047\n",
      "Train Epoch: 2, Loss: 4.611149\n",
      "Train Epoch: 2, Loss: 4.601773\n",
      "Train Epoch: 2, Loss: 4.605679\n",
      "Train Epoch: 2, Loss: 4.612467\n",
      "Train Epoch: 2, Loss: 4.601588\n",
      "Train Epoch: 2, Loss: 4.591951\n",
      "Train Epoch: 2, Loss: 4.608709\n",
      "Train Epoch: 2, Loss: 4.606009\n",
      "Train Epoch: 2, Loss: 4.598076\n",
      "Train Epoch: 2, Loss: 4.604023\n",
      "Train Epoch: 2, Loss: 4.601499\n",
      "Train Epoch: 2, Loss: 4.599303\n",
      "Train Epoch: 2, Loss: 4.602376\n",
      "Train Epoch: 2, Loss: 4.606428\n",
      "Train Epoch: 2, Loss: 4.613337\n",
      "Train Epoch: 2, Loss: 4.605285\n",
      "Train Epoch: 2, Loss: 4.607733\n",
      "Train Epoch: 2, Loss: 4.607654\n",
      "Train Epoch: 2, Loss: 4.607216\n",
      "Train Epoch: 2, Loss: 4.604783\n",
      "Train Epoch: 2, Loss: 4.600661\n",
      "Train Epoch: 2, Loss: 4.606146\n",
      "Train Epoch: 2, Loss: 4.604101\n",
      "Train Epoch: 2, Loss: 4.601151\n",
      "Train Epoch: 2, Loss: 4.603242\n",
      "Train Epoch: 2, Loss: 4.598690\n",
      "Train Epoch: 2, Loss: 4.605371\n",
      "Train Epoch: 2, Loss: 4.608839\n",
      "Train Epoch: 2, Loss: 4.603175\n",
      "Train Epoch: 2, Loss: 4.605511\n",
      "Train Epoch: 2, Loss: 4.603983\n",
      "Train Epoch: 2, Loss: 4.601255\n",
      "Train Epoch: 2, Loss: 4.609714\n",
      "Train Epoch: 2, Loss: 4.606859\n",
      "Train Epoch: 2, Loss: 4.602849\n",
      "Train Epoch: 2, Loss: 4.613732\n",
      "Train Epoch: 2, Loss: 4.610463\n",
      "Train Epoch: 2, Loss: 4.605087\n",
      "Train Epoch: 2, Loss: 4.613034\n",
      "Train Epoch: 2, Loss: 4.611733\n",
      "Train Epoch: 2, Loss: 4.609030\n",
      "Train Epoch: 2, Loss: 4.603541\n",
      "Train Epoch: 2, Loss: 4.598530\n",
      "Train Epoch: 2, Loss: 4.613230\n",
      "Train Epoch: 2, Loss: 4.601331\n",
      "Train Epoch: 2, Loss: 4.605383\n",
      "Train Epoch: 2, Loss: 4.611983\n",
      "Train Epoch: 2, Loss: 4.600340\n",
      "Train Epoch: 2, Loss: 4.605257\n",
      "Train Epoch: 2, Loss: 4.607988\n",
      "Train Epoch: 2, Loss: 4.604170\n",
      "Train Epoch: 2, Loss: 4.599205\n",
      "Train Epoch: 2, Loss: 4.608197\n",
      "Train Epoch: 2, Loss: 4.611601\n",
      "Train Epoch: 2, Loss: 4.599487\n",
      "Train Epoch: 2, Loss: 4.610905\n",
      "Train Epoch: 2, Loss: 4.605729\n",
      "Train Epoch: 2, Loss: 4.607935\n",
      "Train Epoch: 2, Loss: 4.608858\n",
      "Train Epoch: 2, Loss: 4.607295\n",
      "Train Epoch: 2, Loss: 4.609276\n",
      "Train Epoch: 2, Loss: 4.607899\n",
      "Train Epoch: 2, Loss: 4.602917\n",
      "Train Epoch: 2, Loss: 4.607322\n",
      "Train Epoch: 2, Loss: 4.610111\n",
      "Train Epoch: 2, Loss: 4.614336\n",
      "Train Epoch: 2, Loss: 4.605739\n",
      "Train Epoch: 2, Loss: 4.605838\n",
      "Train Epoch: 2, Loss: 4.605811\n",
      "Train Epoch: 2, Loss: 4.608733\n",
      "Train Epoch: 2, Loss: 4.601802\n",
      "Train Epoch: 2, Loss: 4.615291\n",
      "Train Epoch: 2, Loss: 4.598251\n",
      "Train Epoch: 2, Loss: 4.602756\n",
      "Train Epoch: 2, Loss: 4.604371\n",
      "Train Epoch: 2, Loss: 4.605993\n",
      "Train Epoch: 2, Loss: 4.609602\n",
      "Train Epoch: 2, Loss: 4.608039\n",
      "Train Epoch: 2, Loss: 4.601162\n",
      "Train Epoch: 2, Loss: 4.605692\n",
      "Train Epoch: 2, Loss: 4.600886\n",
      "Train Epoch: 2, Loss: 4.601464\n",
      "Train Epoch: 2, Loss: 4.605292\n",
      "Train Epoch: 2, Loss: 4.608026\n",
      "Train Epoch: 2, Loss: 4.604663\n",
      "Train Epoch: 2, Loss: 4.608059\n",
      "Train Epoch: 2, Loss: 4.611169\n",
      "Train Epoch: 2, Loss: 4.599531\n",
      "Train Epoch: 2, Loss: 4.609951\n",
      "Train Epoch: 2, Loss: 4.605358\n",
      "Train Epoch: 2, Loss: 4.602331\n",
      "Train Epoch: 2, Loss: 4.604834\n",
      "Train Epoch: 2, Loss: 4.605564\n",
      "Train Epoch: 2, Loss: 4.601803\n",
      "Train Epoch: 2, Loss: 4.615005\n",
      "Train Epoch: 2, Loss: 4.600490\n",
      "Train Epoch: 2, Loss: 4.604198\n",
      "Train Epoch: 2, Loss: 4.607038\n",
      "Train Epoch: 2, Loss: 4.601024\n",
      "Train Epoch: 2, Loss: 4.605699\n",
      "Train Epoch: 2, Loss: 4.606570\n",
      "Train Epoch: 2, Loss: 4.603037\n",
      "Train Epoch: 2, Loss: 4.605327\n",
      "Train Epoch: 2, Loss: 4.605369\n",
      "Train Epoch: 2, Loss: 4.604968\n",
      "Train Epoch: 2, Loss: 4.614723\n",
      "Train Epoch: 2, Loss: 4.605366\n",
      "Train Epoch: 2, Loss: 4.604353\n",
      "Train Epoch: 2, Loss: 4.604011\n",
      "Train Epoch: 2, Loss: 4.601427\n",
      "Train Epoch: 2, Loss: 4.603451\n",
      "Train Epoch: 2, Loss: 4.612862\n",
      "Train Epoch: 2, Loss: 4.600847\n",
      "Train Epoch: 2, Loss: 4.604548\n",
      "Train Epoch: 2, Loss: 4.609978\n",
      "Train Epoch: 2, Loss: 4.611203\n",
      "Train Epoch: 2, Loss: 4.610517\n",
      "Train Epoch: 2, Loss: 4.601161\n",
      "Train Epoch: 2, Loss: 4.597249\n",
      "Train Epoch: 2, Loss: 4.600542\n",
      "Train Epoch: 2, Loss: 4.608191\n",
      "Train Epoch: 2, Loss: 4.605674\n",
      "Train Epoch: 2, Loss: 4.608559\n",
      "Train Epoch: 2, Loss: 4.605256\n",
      "Train Epoch: 2, Loss: 4.603427\n",
      "Train Epoch: 2, Loss: 4.605199\n",
      "Train Epoch: 2, Loss: 4.602225\n",
      "Train Epoch: 2, Loss: 4.610622\n",
      "Train Epoch: 2, Loss: 4.606843\n",
      "Train Epoch: 2, Loss: 4.605881\n",
      "Train Epoch: 2, Loss: 4.600973\n",
      "Train Epoch: 2, Loss: 4.604938\n",
      "Train Epoch: 2, Loss: 4.594840\n",
      "Train Epoch: 2, Loss: 4.614604\n",
      "Train Epoch: 2, Loss: 4.607997\n",
      "Train Epoch: 2, Loss: 4.610915\n",
      "Train Epoch: 2, Loss: 4.605494\n",
      "Train Epoch: 2, Loss: 4.597324\n",
      "Train Epoch: 2, Loss: 4.604139\n",
      "Train Epoch: 2, Loss: 4.597607\n",
      "Train Epoch: 2, Loss: 4.611966\n",
      "Train Epoch: 2, Loss: 4.597617\n",
      "Train Epoch: 2, Loss: 4.612224\n",
      "Train Epoch: 2, Loss: 4.604984\n",
      "Train Epoch: 2, Loss: 4.598725\n",
      "Train Epoch: 2, Loss: 4.609098\n",
      "Train Epoch: 2, Loss: 4.609745\n",
      "Train Epoch: 2, Loss: 4.607801\n",
      "Train Epoch: 2, Loss: 4.601705\n",
      "Train Epoch: 2, Loss: 4.602493\n",
      "Train Epoch: 2, Loss: 4.606414\n",
      "Train Epoch: 2, Loss: 4.609179\n",
      "Train Epoch: 2, Loss: 4.604523\n",
      "Train Epoch: 2, Loss: 4.602075\n",
      "Train Epoch: 2, Loss: 4.608959\n",
      "Train Epoch: 2, Loss: 4.603709\n",
      "Train Epoch: 2, Loss: 4.596293\n",
      "Train Epoch: 2, Loss: 4.599935\n",
      "Train Epoch: 2, Loss: 4.607018\n",
      "Train Epoch: 2, Loss: 4.605486\n",
      "Train Epoch: 2, Loss: 4.608650\n",
      "Train Epoch: 2, Loss: 4.602951\n",
      "Train Epoch: 2, Loss: 4.615231\n",
      "Train Epoch: 2, Loss: 4.599874\n",
      "Train Epoch: 2, Loss: 4.602704\n",
      "Train Epoch: 2, Loss: 4.604720\n",
      "Train Epoch: 2, Loss: 4.606572\n",
      "Train Epoch: 2, Loss: 4.603515\n",
      "Train Epoch: 2, Loss: 4.608803\n",
      "Train Epoch: 2, Loss: 4.597489\n",
      "Train Epoch: 2, Loss: 4.604348\n",
      "Train Epoch: 2, Loss: 4.610958\n",
      "Train Epoch: 2, Loss: 4.608568\n",
      "Train Epoch: 2, Loss: 4.606430\n",
      "Train Epoch: 2, Loss: 4.610169\n",
      "Train Epoch: 2, Loss: 4.596897\n",
      "Train Epoch: 2, Loss: 4.607508\n",
      "Train Epoch: 2, Loss: 4.602043\n",
      "Train Epoch: 2, Loss: 4.598868\n",
      "Train Epoch: 2, Loss: 4.605265\n",
      "Train Epoch: 2, Loss: 4.607571\n",
      "Train Epoch: 2, Loss: 4.605437\n",
      "Train Epoch: 2, Loss: 4.598937\n",
      "Train Epoch: 2, Loss: 4.610390\n",
      "Train Epoch: 2, Loss: 4.590871\n",
      "Train Epoch: 2, Loss: 4.598425\n",
      "Train Epoch: 2, Loss: 4.609811\n",
      "Train Epoch: 2, Loss: 4.602797\n",
      "Train Epoch: 2, Loss: 4.606011\n",
      "Train Epoch: 2, Loss: 4.615223\n",
      "Train Epoch: 2, Loss: 4.607109\n",
      "Train Epoch: 2, Loss: 4.604090\n",
      "Train Epoch: 2, Loss: 4.611045\n",
      "Train Epoch: 2, Loss: 4.601911\n",
      "Train Epoch: 2, Loss: 4.599186\n",
      "Train Epoch: 2, Loss: 4.612101\n",
      "Train Epoch: 2, Loss: 4.609330\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1c04de8558e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastml/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Github/vgg-from-torch/vgg/model/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv13\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv14\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv15\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastml/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastml/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastml/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    413\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 415\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    416\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_step == 0:\n",
    "           print(\"Train Epoch: {}, Loss: {:.6f}\".format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
